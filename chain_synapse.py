import os
import requests
import json
from datetime import datetime, timedelta
import time
import statistics
from dotenv import load_dotenv

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞
load_dotenv()

# API –∫–ª—é—á–∏
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
CRYPTO_PANIC_API_KEY = os.getenv("CRYPTO_PANIC_API_KEY")

# –ú–æ–Ω–µ—Ç—ã –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è (—Ç–∏–∫–µ—Ä: [—Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –Ω–∞ GitHub, –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞])
TARGET_ASSETS = {
    "DOT": ["polkadot-sdk/polkadot", "Polkadot"],
    "SOL": ["solana-labs/solana", "Solana"],
    "ATOM": ["cosmos/cosmos-sdk", "Cosmos ATOM"],
    "RNDR": ["RenderToken/rndr-app", "Render Token"],
}

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞
HISTORY_FILE = "synapse_history.json" # –§–∞–π–ª –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
HOURS_TO_ANALYZE = 24 # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞
ANOMALY_STD_DEV_THRESHOLD = 2.0 # –ü–æ—Ä–æ–≥ –¥–ª—è –∞–Ω–æ–º–∞–ª–∏–∏ (–≤–æ —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —Å—Ç.–æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø—Ä–µ–≤—ã—à–µ–Ω–æ)
CONVERGENCE_THRESHOLD = 2 # –°–∫–æ–ª—å–∫–æ –∞–Ω–æ–º–∞–ª–∏–π –Ω—É–∂–Ω–æ –¥–ª—è "—Å–∏–≥–Ω–∞–ª–∞ –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏"

# --- –ò–ù–°–¢–†–£–ú–ï–ù–¢–´ API ---

def get_github_commits(repo: str) -> int:
    """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–º–∏—Ç–æ–≤ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞."""
    since_date = (datetime.utcnow() - timedelta(hours=HOURS_TO_ANALYZE)).isoformat()
    url = f"https://api.github.com/repos/{repo}/commits"
    headers = {"Authorization": f"token {GITHUB_TOKEN}"} if GITHUB_TOKEN else {}
    params = {"since": since_date, "per_page": 100}
    try:
        response = requests.get(url, headers=headers, params=params, timeout=10)
        response.raise_for_status()
        # API –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–æ 100 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É, –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã —Å—á–∏—Ç–∞–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        return len(response.json())
    except requests.RequestException as e:
        print(f"  [Error] GitHub API request failed for {repo}: {e}")
        return 0

def get_news_mentions(query: str) -> int:
    """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –≤ –Ω–æ–≤–æ—Å—Ç—è—Ö —Å CryptoPanic."""
    if not CRYPTO_PANIC_API_KEY:
        print("  [Warning] CRYPTO_PANIC_API_KEY not set. Skipping news analysis.")
        return 0
    url = f"https://cryptopanic.com/api/v1/posts/?auth_token={CRYPTO_PANIC_API_KEY}&search={query}&public=true"
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        posts = response.json().get("results", [])
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ—Å—Ç—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞
        now = datetime.utcnow()
        recent_posts = [
            p for p in posts 
            if now - datetime.fromisoformat(p['created_at'].replace('Z', '')) < timedelta(hours=HOURS_TO_ANALYZE)
        ]
        return len(recent_posts)
    except requests.RequestException as e:
        print(f"  [Error] CryptoPanic API request failed for {query}: {e}")
        return 0

def get_reddit_mentions(query: str, subreddit: str = "cryptocurrency") -> int:
    """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –Ω–∞ Reddit (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫)."""
    # –í–Ω–∏–º–∞–Ω–∏–µ: —ç—Ç–æ –æ—á–µ–Ω—å —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥. –î–ª—è —Å–µ—Ä—å–µ–∑–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PRAW –∏ Pushshift API.
    url = f"https://www.reddit.com/r/{subreddit}/search.json"
    headers = {'User-agent': 'ChainSynapse Bot 0.1'}
    params = {'q': query, 'sort': 'new', 't': 'day', 'restrict_sr': 'on'}
    try:
        response = requests.get(url, headers=headers, params=params, timeout=10)
        response.raise_for_status()
        posts = response.json()['data']['children']
        return len(posts)
    except (requests.RequestException, KeyError) as e:
        print(f"  [Error] Reddit search failed for {query}: {e}")
        return 0

# --- –õ–û–ì–ò–ö–ê –ê–ù–ê–õ–ò–ó–ê ---

def load_history() -> dict:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –∏–∑ JSON —Ñ–∞–π–ª–∞."""
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, 'r') as f:
            return json.load(f)
    return {}

def save_history(history: dict):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –≤ JSON —Ñ–∞–π–ª."""
    with open(HISTORY_FILE, 'w') as f:
        json.dump(history, f, indent=2)

def analyze_anomaly(data_points: list, current_value: int) -> tuple[bool, float, float]:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–µ–π."""
    if len(data_points) < 5: # –ù—É–∂–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        return False, 0.0, 0.0
    
    mean = statistics.mean(data_points)
    stdev = statistics.stdev(data_points)
    
    if stdev == 0: # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
        return False, mean, stdev

    is_anomaly = current_value > (mean + ANOMALY_STD_DEV_THRESHOLD * stdev)
    return is_anomaly, mean, stdev

def run_analysis():
    """–ì–ª–∞–≤–Ω—ã–π —Ü–∏–∫–ª –∞–Ω–∞–ª–∏–∑–∞."""
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] --- Starting ChainSynapse Analysis ---")
    history = load_history()
    convergence_alerts = []

    for ticker, (repo, query) in TARGET_ASSETS.items():
        print(f"[INFO] Analyzing {query} ({ticker})...")
        
        if ticker not in history:
            history[ticker] = {"github": [], "news": [], "reddit": []}

        # 1. –°–±–æ—Ä —Å–≤–µ–∂–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        github_commits = get_github_commits(repo)
        news_mentions = get_news_mentions(query)
        reddit_mentions = get_reddit_mentions(query)
        time.sleep(2) # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—à–∞—Ç—å –ª–∏–º–∏—Ç—ã API

        # 2. –ê–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π
        sources = {
            "GitHub": (github_commits, history[ticker]["github"]),
            "News": (news_mentions, history[ticker]["news"]),
            "Reddit": (reddit_mentions, history[ticker]["reddit"]),
        }
        
        anomalies_found = []
        for name, (current, past_data) in sources.items():
            is_anomaly, mean, stdev = analyze_anomaly(past_data, current)
            status = "-> ANOMALY DETECTED!" if is_anomaly else "-> Normal"
            print(f"  [{name}] Activity in last {HOURS_TO_ANALYZE}h: {current} (Baseline avg: {mean:.1f}, StDev: {stdev:.1f}) {status}")
            if is_anomaly:
                anomalies_found.append(name)
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏—é
        if len(anomalies_found) >= CONVERGENCE_THRESHOLD:
            convergence_alerts.append((query, ticker, anomalies_found))

        # 4. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
        history[ticker]["github"].append(github_commits)
        history[ticker]["news"].append(news_mentions)
        history[ticker]["reddit"].append(reddit_mentions)
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é, —á—Ç–æ–±—ã –æ–Ω–∞ –Ω–µ —Ä–æ—Å–ª–∞ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ
        for key in history[ticker]:
            history[ticker][key] = history[ticker][key][-30:]

    # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
    save_history(history)

    # 6. –í—ã–≤–æ–¥ –æ—Ç—á–µ—Ç–∞ –æ –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏
    if convergence_alerts:
        print("\n" + "-"*50)
        for query, ticker, platforms in convergence_alerts:
            print(f"!!! üß†üîó CONVERGENCE ALERT: {query} ({ticker}) üîóüß† !!!")
            print(f"Anomalies detected on {len(platforms)} platforms: {platforms}")
            print("This could indicate a significant organic event. DYOR!")
        print("-" * 50)
    
    print("\n--- Analysis Complete ---")


if __name__ == "__main__":
    run_analysis()
